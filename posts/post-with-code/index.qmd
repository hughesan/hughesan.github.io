---
title: "Where does VDOT come from, and what's up with point estimates in pace prediction? Diving into sampling variability in running performances"
author: "Alex Hughes"
date: "2024-12-15"
date-modified: "2024-12-15"
categories: [running, stats]
image: "image.jpg"
draft: true
---

```{r message = FALSE, warning = FALSE}
library(dplyr)
```

On the r/AdvancedRunning subreddit, a question that is frequently asked goes something like this: "I ran a marathon in 3:31 but my watch says I actually ran 26.57 miles, so did I run 26.2 in less than 3:30?" or closely related, "Do I enter my watch time and distance or the chip time and official course distance" into a race prediction/equivalent time calculator such as [vdot](https://vdoto2.com/calculator/).

Every time, in turn, the community offers a resounding no. Use the official time, and the official distance, and that's the only information you're allowed to take away from the day. You enter this singular time into the vdot calculator and it gives you singular paces back. For example, our 3:31 marathoner should do threshold work at 7:33 min/mi pace and could reasonably target a 45:52 10K.  

There's seemingly no room in this model of training for the concept of sampling variability, which is to say, if you could run a race multiple times in parallel, how much variation would there be in your finish times simply due to randomness? If we understand and account for this uncertainty, we can walk away from a race with a lot more information about our performance than singular training and target paces.  

Maybe you've run a packed race before where the first couple miles were heavily dictated by the crowd around you, but that same crowd also blocked the wind, and perhaps those slower early miles allowed you to pick it up later in the race. Chances are in this parallel experiment, your times would vary, but by how much?  


```{r}
# mdsr::Cherry %>%o
#   group_by(name.yob) 
```
